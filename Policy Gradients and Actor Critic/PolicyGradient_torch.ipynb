{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab specific cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt update\n",
    "!pip install pyvirtualdisplay\n",
    "!apt-get install -y xvfb python-opengl ffmpeg\n",
    "!pip install gym[atari,box2d,classic_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "\n",
    "# root_path = 'gdrive/My Drive/Colab Notebooks/RL/'\n",
    "# import os\n",
    "# os.chdir(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorflow_version 2.x\n",
    "# %tensorflow_version 1.x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import io\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "import base64\n",
    "\n",
    "\"\"\"\n",
    "Utility functions to enable video recording of gym environment and displaying it\n",
    "To enable video, just do \"env = wrap_env(env)\"\"\n",
    "\"\"\"\n",
    "\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[-1]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "\n",
    "        # you can add \"loop\" after autoplay to keep the video looping after it ends\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                     controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "                 </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "\n",
    "        \n",
    "\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "def wrap_env(env):\n",
    "    env = Monitor(env, './video', force=True)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If not using colab change the value to false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using_colab = False\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if \"cuda\" in str(device):\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    print(\"Not using GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Algorithm (REINFORCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import logger as gymlogger\n",
    "gymlogger.set_level(40) #error only\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_list = [\"LunarLander-v2\", \"MsPacman-ram-v0\", \"CartPole-v0\", \"MountainCar-v0\",\n",
    "            \"Breakout-ram-v4\", \"Acrobot-v1\"]\n",
    "\n",
    "env_to_use = env_list[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the environment and exploring it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_to_use)\n",
    "if using_colab:\n",
    "    env = wrap_env(env)\n",
    "\n",
    "#check out the environment's action and observation space!\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "\n",
    "from gym import spaces\n",
    "assert type(env.observation_space)==spaces.Box, print(\"State space should be continuous\")\n",
    "assert len(env.observation_space.shape)==1, print(\"State space should be 1-D\")\n",
    "assert type(env.action_space)==spaces.Discrete, print(\"Action space should be discrete\")\n",
    "\n",
    "observation = env.reset()\n",
    "while True:\n",
    "    env.render()\n",
    "    #your agent goes here\n",
    "    action = env.action_space.sample() # selecting a random action from the action space\n",
    "    observation, reward, done, info = env.step(action) \n",
    "    if done: \n",
    "        break\n",
    "    # break\n",
    "\n",
    "env.close()\n",
    "if using_colab:\n",
    "    show_video()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Network to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class PGLoss(nn.Module):\n",
    "    def forward(self, policy, act, rew_wt):\n",
    "        logp = policy.log_prob(act)\n",
    "        return -(logp * rew_wt).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, batch_size, device='cpu'):\n",
    "        self.env = env\n",
    "        self.device = device\n",
    "        \n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.n\n",
    "\n",
    "        self.policy_network = PolicyNet(self.state_size, self.action_size, 4).to(device)\n",
    "        self.loss = PGLoss()\n",
    "        self.batch_size = batch_size\n",
    "        self.train_stats = defaultdict(list)\n",
    "\n",
    "    def get_policy(self, obs):\n",
    "        net_op = self.policy_network(torch.as_tensor(obs, dtype=torch.float32).to(self.device))\n",
    "        return Categorical(logits=net_op.cpu())\n",
    "    def get_action(self, obs):\n",
    "        return self.get_policy(obs).sample().item()\n",
    "\n",
    "    def get_episode_batch(self):\n",
    "        batch_obs = []\n",
    "        batch_action = []\n",
    "        batch_weights = []\n",
    "        batch_returns = []\n",
    "        batch_lengths = []\n",
    "\n",
    "        cur_obs = self.env.reset()\n",
    "        episode_rewards = []\n",
    "        done = False\n",
    "\n",
    "        while True:\n",
    "            batch_obs.append(cur_obs.copy())\n",
    "            \n",
    "            cur_action = self.get_action(cur_obs)\n",
    "            cur_obs, cur_reward, done, _ = self.env.step(cur_action)\n",
    "\n",
    "            batch_action.append(cur_action)\n",
    "            episode_rewards.append(cur_reward)\n",
    "            \n",
    "            if done:\n",
    "                episode_return, episode_length = sum(episode_rewards), len(episode_rewards)\n",
    "                \n",
    "                batch_returns.append(episode_return)\n",
    "                batch_lengths.append(episode_length)\n",
    "                \n",
    "                def get_reward_to_go(rewards_list):\n",
    "                    rtg = []\n",
    "                    sum_rewards = 0\n",
    "                    for i in rewards_list[::-1]:\n",
    "                        sum_rewards+=i\n",
    "                        rtg.append(sum_rewards)\n",
    "                    return rtg[::-1]\n",
    "                batch_weights += get_reward_to_go(episode_rewards)\n",
    "                \n",
    "                if len(batch_obs) > self.batch_size:\n",
    "                    break\n",
    "                \n",
    "                cur_obs, done, episode_rewards = self.env.reset(), False, []\n",
    "        return batch_obs, batch_action, batch_weights, batch_returns, batch_lengths\n",
    "\n",
    "    def train(self, epochs):\n",
    "        optimizer = optim.Adam(self.policy_network.parameters(), lr=1e-2)\n",
    "        for i in range(epochs):\n",
    "            obs, act, rew_wt, eps_rew, eps_len = self.get_episode_batch()\n",
    "            obs = torch.as_tensor(obs)\n",
    "            act = torch.as_tensor(act)\n",
    "            rew_wt = torch.as_tensor(rew_wt)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = self.loss(self.get_policy(obs), act, rew_wt)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            self.train_stats[\"total_loss\"] += [loss.item()]\n",
    "            self.train_stats[\"rewards\"] += [np.mean(eps_rew)]\n",
    "\n",
    "            print(\"Epoch:\", i, loss.item(), np.mean(eps_rew), np.mean(eps_len))\n",
    "    \n",
    "    def plot_train_stats(self):\n",
    "        if len(self.train_stats)==0:\n",
    "            print(\"first train to print train stats\")\n",
    "        for i in self.train_stats:\n",
    "            plt.plot(self.train_stats[i])\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(i)\n",
    "            plt.show()\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Agent's instance and using it to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"GPU available:\", torch.cuda.is_available())\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = gym.make(env_to_use)\n",
    "if using_colab:\n",
    "    env = wrap_env(env)\n",
    "\n",
    "agent = Agent(env, batch_size=5000, device=device)\n",
    "agent.train(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.plot_train_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watch the trained agent\n",
    "env = gym.make(env_to_use)\n",
    "if using_colab:\n",
    "    env = wrap_env(env)\n",
    "state = env.reset()\n",
    "done=False\n",
    "while not done:\n",
    "    action = agent.get_action(state)\n",
    "    env.render()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "env.close()\n",
    "if using_colab:\n",
    "    show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rl] *",
   "language": "python",
   "name": "conda-env-rl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
